# 2.1 二分类(Binary Classification)

我们知道逻辑回归是一个用于二分类(**binary classification**)的算法。。
二分类就是输出y只有{0,1}两个离散值（也有{-1,1}的情况）。 我们以一个图像识别问题为例，判断图片中是否有猫存在，0代表noncat，1代表cat。其输入是由特征向量𝑥表示的图像，并预测相应的标签𝑦是1还是0。
![](assets/2神经网络的编程基础.png)
在计算机中为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道，三个矩阵与图像大小相同，如果图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。
![](assets/2神经网络的编程基础_1.png)

单元格中的值表示将用于创建 n 维特征向量的像素强度。在模式识别和机器学习中，一个特征向量代表一个图像。为了创建特征向量 𝑥，每种颜色的像素强度值将被“展开”或“重塑”。输入特征向量𝑥的维度为𝑛= 64\*64\*3 = 12288。

![](assets/2神经网络的编程基础_2.png)

所以在二分类问题中，我们的目标是学习一个分类器：输入一幅以特征向量𝑥表示的图像，并预测相应的输出标签𝑦是1还是0。

**Notation**
- 样本： $(x,y)$, 训练样本包含$m$个； 
- 其中$x\in R^{n_x}$,表示样本$x$ 包含$n_x$个特征；
- $y\in0,1$,目标值属于0、1分类；
- m个训练数据:$\left\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(m)},y^{(m)})\right\}$

输入神经网络时样本数据的形状为：
![](assets/2神经网络的编程基础_5.png)

# 2.2 逻辑回归(Logistic Regression)

接下来我们就来介绍如何使用逻辑回归来解决二分类问题。

逻辑回归中，给定输入 $x$, 预测值 $\hat{y}=P(y=1|x),\mathrm{where}0\leq\hat{y}\leq1$ 表示为1的概率，取值范围在\[0,1\]之间。

这是其与二分类模型不同的地方。逻辑回归的预测值为一个连续的概率。

Logistic回归中使用的参数是：
- 输入特征向量：$x\in\mathbb{R}^{n_x}$ ，其中 $𝑛_𝑥$ 是特征数量 
- 训练标签：𝑦 ε 0,1 
- 权重：$w\in\mathbb{R}^{n_x}$ ，其中$𝑛_𝑥$ 是特征数量 
- 阈值：𝑏 ε ℝ 
- 输出：$\hat{y}=\sigma(w^{T}x+b)$
- Sigmoid 函数：$s=\sigma(w^{T}x+b)=\sigma(z){=}\frac{1}{1+e^{-z}}$

$w^{T}x+b$是一个线性函数 (𝑎𝑥 + 𝑏)，其输出为一个连续的概率。但由于我们正在寻找 \[0,1\] 之间的概率约束，因此使用 sigmoid 函数将输出边界限制在在\[0,1\] 之间，如下图所示。
![](assets/2神经网络的编程基础_6.png)

sigmoid函数的公式为：$\sigma(z){=}\frac{1}{1+e^{-z}}$

其中，z是一个实数。

需要注意以下几点：

1. 当z非常大时，sigmoid函数接近于1。这是因为指数$e^{-x}$会变得非常小，接近于0，所以1除以1加上一个接近于0的项，结果就会接近于1。

2. 相反地，当x非常小或为一个绝对值很大的负数时，sigmoid函数接近于0。可以将其看作是1除以1加上一个非常非常大的数，结果就会接近于0。

3. 实际上，当x变成一个绝对值很大的负数时，sigmoid函数会非常接近于0。

因此，在实现逻辑回归时，我们的目标是通过调整机器学习参数w和b，使得sigmoid函数能够很好地估计给定情况的概率。

现在你已经知道逻辑回归模型是什么样子了，下一步要做的是使用代价函数训练参数w和参数b。
# 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）

为了训练参数𝑤和𝑏，我们需要定义一个成本函数。

## 损失（误差）函数
损失函数测量预测 $(\hat{y}^{(i)})$ 和期望输出 $({y}^{(i)})$之间的差异。
换句话说，损失函数计算单个训练示例的误差。

对于逻辑回归，我们一般不使用平方误差$L(\hat{y},y)=\frac12(\hat{y}-y)^2$来作为Loss function。因为上面的平方误差损失函数一般是非凸函数（non-convex），其在使用梯度下降算法的时候，容易得到局部最优解，而不是全局最优解，因此要选择凸函数，所以我们在逻辑回归模型中定义另外一个损失函数。
  $$L\left(\hat{y},y\right)=-y\log(\hat{y})-(1-y)\log(1-\hat{y})$$

当$y=1$时，$L(\hat{y},y)=-\log\hat{y}$。如果$\hat{y}$越接近1，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近0， $L(\hat{y},y)\approx+\infty$,表示预测效果越差；

当$y=0$时，$L(\hat{y},y)=-\log(1-\hat{y})$。如果$\hat{y}$越接近0，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近1，$L(\hat{y},y)\approx+\infty$,表示预测效果越差；

我们的目标是最小化样本点的损失Loss Function, 损失函数是针对单个样本点的。


## 成本函数
Loss function是针对单个样本的，成本函数是整个训练集的损失函数的平均值。
$$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L\big(\hat{y}^{(i)},y^{(i)}\big)=-\frac{1}{m}\sum_{i=1}^{m}\big[(y^{(i)}\log\big(\hat{y}^{(i)}\big)+\big(1-y^{(i)}\big)\log(1-\hat{y}^{(i)})\big]$$
# 2.4 梯度下降（Gradient Descent）

# 2.5 导数（Derivatives）

# 2.6 更多的导数例子（More Derivative Examples）

# 2.7 计算图（Computation Graph）

整个神经网络的训练过程实际上包含了两个过程：正向传播（Forward Propagation）和反向传播（Back Propagation）。

正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，利用输出计算出对应的梯度或导数，对参数w和b计算梯度的过程。

下面，我们用计算图（Computation graph）的形式来理解这两个过程。


举个简单的例子，假如Cost function为J(a,b,c)=3(a+bc)，包含a，b，c三个变量。
我们用u表示bc，v表示a+u，则J=3v。
它的计算图可以写成如下图所示：
![](assets/2神经网络的编程基础_7.png)

首先计算u=bc，接着计算v=a+u，最后计算J=3v

计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。


# 2.8 计算图导数（Derivatives with a Computation Graph）

上一部分介绍的是计算图的正向传播（Forward Propagation），下面我们来介绍其反向传播（Back Propagation），即计算输出对输入的偏导数。

还是上个计算图的例子，输入参数有3个，分别是a，b，c。
![](assets/2神经网络的编程基础_8.png)
首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数。则利用求导技巧，可以得到：
$\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial a}=3\cdot1=3$

然后计算J对参数b的偏导数。从计算图上来看，从右到左，J是v的函数，v是u的函数，u是b的函数。可以推导：
$\frac{\partial J}{\partial b}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial b}=3\cdot1\cdot c=3\cdot1\cdot2=6$

最后计算J对参数c的偏导数。仍从计算图上来看，从右到左，J是v的函数，v是u的函数，u是c的函数。可以推导：
$\frac{\partial J}{\partial c}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial c}=3\cdot1\cdot b=3\cdot1\cdot3=9$

# 2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）




**总结：**
首先知道参数w和b，利用正向传播计算出损失函数的值，接着根据损失函数的值利用反向传播计算出w和b的导数，最后利用计算出的导数更新w和b的值
$$\begin{gathered}
w=w-\alpha\frac{\partial J(w,b)}{\partial w} \\
b=b-\alpha\frac{\partial J(w,b)}{\partial b} 
\end{gathered}$$
依次循环，知道损失函数最小。
# 2.10 m个样本的梯度下降的例(Gradient Descent on m Examples)

![](assets/2神经网络的编程基础_9.png)
$\mathrm{d}\omega_{1}$ 、$\mathrm{d}\omega_{2}$ 用于累加，单个样本中的偏导数都累加在此，最后求平均，所以初始化为0。 
# 2.11 向量化(Vectorization)

# 2.12 更多的向量化例子（More Examples of Vectorization）

# 2.13 向量化逻辑回归(Vectorizing Logistic Regression)

# 2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression's Gradient）

# 2.15 Python中的广播机制（Broadcasting in Python）

# 2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）

# 2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）

# 2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）