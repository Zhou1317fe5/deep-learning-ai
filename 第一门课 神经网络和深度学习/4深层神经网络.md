# 1 深层神经网络

深层神经网络其实就是包含更多的隐藏层神经网络。

![](assets/4深层神经网络.png)

# 2 深层神经网络正向传播

接下来，我们来推导一下深层神经网络的正向传播过程。
![](assets/4深层神经网络_1.png)

以4层神经网络为例，对于单个样本。

第1层，l=1：
$$\begin{gathered}z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}\\a^{[1]}=g^{[1]}(z^{[1]})\end{gathered}$$

第2层，l=2：
$$\begin{gathered}z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\a^{[2]}=g^{[2]}(z^{[2]})\end{gathered}$$

第3层，l=3：
$$\begin{gathered}z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}\\a^{[3]}=g^{[3]}(z^{[3]})\end{gathered}$$

第4层，l=4：
$$\begin{gathered}z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}\\a^{[4]}=g^{[4]}(z^{[4]})\end{gathered}$$

如果有m个训练样本，其向量化矩阵形式为：

第1层，l=1：
$$\begin{gathered}Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\A^{[1]}=g^{[1]}(Z^{[1]})\end{gathered}$$

第2层，l=2：
$$\begin{gathered}Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\A^{[2]}=g^{[2]}(Z^{[2]})\end{gathered}$$

第3层，l=3：
$$\begin{gathered}Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}\\A^{[3]}=g^{[3]}(Z^{[3]})\end{gathered}$$

第4层，l=4：
$$\begin{gathered}Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}\\A^{[4]}=g^{[4]}(Z^{[4]})\end{gathered}$$

因此，向前传播：
- 公式：
$$\begin{array}{c}\mathrm{z^{[l]}=W^{[l]}\cdot a^{[l-1]}+b^{[l]}}\\\mathrm{a^{[l]}=g^{[l]}(z^{[l]})}\end{array}$$

- 向量化：
$$\begin{gathered}Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\A^{[l]}=g^{[l]}(Z^{[l]})\end{gathered}$$
其中l=1,⋯,L

# 3 正确处理矩阵维度

**单个训练样本**
对于单个训练样本，输入x的维度是( $n^{[0]},1$ ) 

参数$W^{[l]}$和$b^{[l]}$的维度分别是 :
$$\begin{gathered}W^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]}:(n^{[l]},1)\end{gathered}$$

反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：
$$\begin{gathered}dW^{[l]}:(n^{[l]},n^{[l-1]})\\db^{[l]}:(n^{[l]},1)\end{gathered}$$
正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：
$$\begin{aligned}z^{[l]}&:&(n^{[l]},1)\\a^{[l]}&:&(n^{[l]},1)\end{aligned}$$

**m个训练样本**
对于m个训练样本，输入矩阵X的维度是( $n^{[0]},m$ )。

$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：
$$\begin{gathered}W^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]}:(n^{[l]},1)\end{gathered}$$
$dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$的相同

$Z^{[l]}$和$A^{[l]}$的维度分别是：
$$\begin{aligned}Z^{[l]}&:&(n^{[l]},m)\\A^{[l]}&:&(n^{[l]},m)\end{aligned}$$
$dZ^{[l]}$ 和 $dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同

# 5 深层神经网络流程块

下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第l层来说，正向传播过程中：
![](assets/4深层神经网络_2.png)


![](assets/4深层神经网络_3.png)

这是第l层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：
![](assets/4深层神经网络_4.png)
![](assets/4深层神经网络_5.png)

# 6 神经网络参数和超参数

神经网络中的参数（parameters）就是我们熟悉的$W^{[l]}$和$b^{[l]}$。而超参数（hyperparameters）则是例如学习速率$\alpha$，训练迭代次数 N,神经网络层数L,各层神经元个数$n^{[l]}$，激活函数$g(z)$等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值。